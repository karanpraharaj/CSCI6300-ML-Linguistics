{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LING 4100/5800 Machine Learning and Linguistics\n",
    "\n",
    "# Exercise: calculating word embedding similarity, finding the closest word, and finding the best analogy\n",
    "\n",
    "\n",
    "## Cosine distance\n",
    "\n",
    "Recall that cosine distance, which we use to compare similarity of vectors representing words, is defined as follows:\n",
    "\n",
    "$$sim({\\bf x}, {\\bf y}) = \\frac{{\\bf x} \\cdot {\\bf y}}{||{\\bf x}||||{\\bf y}||}$$\n",
    "\n",
    "where ${\\bf x} \\cdot {\\bf y}$ is the standard dot product of two vectors and $||{\\bf x}||$ and $||{\\bf y}||$ are the norms of the individual vectors. This needs to be implemented in Python. The dot product function we already have from before, so that's included below.\n",
    "\n",
    "### Norm\n",
    "\n",
    "Additionally, recall that the norm (length) for some vector ${\\bf z}$ was defined as:\n",
    "\n",
    "$$||{\\bf z}|| = \\sqrt{(z_1)^2 + (z_2)^2 + \\ldots + (z_n)^2}$$\n",
    "\n",
    "That is, we simply take the square root of the summed squares of the individual components in the vector. You'll also need to implement this.\n",
    "\n",
    "### Most similar word\n",
    "\n",
    "If we have a word (say *cat*), to find the most similar word among all vectors, we would simply loop over all word representations ${\\bf x}$ in our vocabulary and find one that has maximum similarity to the vector for *cat*:\n",
    "\n",
    "$$sim({\\bf v}_{\\rm cat}, {\\bf x})$$\n",
    "\n",
    "We of course don't compare *cat* with *cat* since that would trivially always have the highest similarity. But we do compare *cat* with every other word in the vocabulary.\n",
    "\n",
    "## Analogies\n",
    "\n",
    "![Image of Vectors](https://adriancolyer.files.wordpress.com/2016/04/word2vec-king-queen-vectors.png?w=300)\n",
    "\n",
    "For calculating an analogy a:b::c:x for some unknown ${\\bf x}$, we want to find the ${\\bf x}$ such that the following quantity is maximized:\n",
    "\n",
    "$$sim({\\bf x}, {\\bf b} - {\\bf a} + {\\bf c})$$\n",
    "\n",
    "For example, we'd expect that if ${\\bf a}$ is the vector representation for *king*, ${\\bf b}$ for *queen* and ${\\bf c}$ for *man*, then the best ${\\bf x}$ would be the vector for *woman*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sub-exercise 1: similarity\n",
    "\n",
    "Here, you should define a function _sim()_ that takes two vectors as arguments, constructed from the above information. To do that, it's best to also define a helper function _norm()_ that takes a single vector and returns its norm.\n",
    "\n",
    "If your _sim_-function is correctly implemented, it should return the following, for example:\n",
    "\n",
    "```\n",
    "sim(vec['cat'], vec['dog'])\n",
    "0.9638665904796965\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import sqrt # You'll need this function\n",
    "\n",
    "# Some made-up toy word vectors\n",
    "vec = {}\n",
    "vec['man'] = [1,2,3,4,5]\n",
    "vec['woman'] = [6,7,8,9,10]\n",
    "vec['king'] = [-5,-4,-3,-2,-1]\n",
    "vec['queen'] = [0,1,2,3,4]\n",
    "vec['dog'] = [-10.2,-3.2,-2.3,-4.3,3.1]\n",
    "vec['cat'] = [-8.3,-3.01,-2.0,-1.3,1.1]\n",
    "\n",
    "# Old definitions that are useful\n",
    "\n",
    "def dp(x,y):       # Dot product\n",
    "    return sum(a*b for a,b in zip(x,y))\n",
    "\n",
    "def vec_sub(x,y):  # Vector subtraction\n",
    "    return [a - b for a,b in zip(x,y)]\n",
    "\n",
    "def vec_add(x,y):  # Vector addition\n",
    "    return [a + b for a,b in zip(x,y)]\n",
    "\n",
    "### Your code here ###\n",
    "\n",
    "# def norm(x):\n",
    "#    Your code here\n",
    "\n",
    "# def sim(x,y):\n",
    "    # Your code here\n",
    "    \n",
    "# sim(vec['cat'], vec['dog'])\n",
    "# should produce 0.9638665904796965 if correctly implemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sub-exercise 2: find closest word\n",
    "\n",
    "Here, you should define a function `find_closest()` that takes a word and the dictionary of vectors as its two arguments and returns the closest word from among the vectors and its score. The reason we need to pass the dictionary as argument to the function is that we need to loop over all word vectors to find the closest one.\n",
    "\n",
    "Your function should behave as follows:\n",
    "\n",
    "```\n",
    "find_closest('cat', vec)\n",
    "('dog', 0.9638665904796965)\n",
    "```\n",
    "\n",
    "Note that as you loop over the vectors, you should not compare a word with itself, i.e. `sim(v[cat], v[cat])` since that will always have the maximum similarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Your code here ###\n",
    "\n",
    "# def find_closest(word, v):\n",
    "    # Your code here\n",
    "\n",
    "# find_closest('cat', vec)\n",
    "# should produce ('dog', 0.9638665904796965)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sub-exercise 3: analogy\n",
    "\n",
    "Here, you should define a function `analogy()`-that takes four arguments where the first three are words, and the last is the dictionary of all vectors. Again, we're passing the vector dictionary to the function so it can make use of it.\n",
    "\n",
    "Your function should work as follows:\n",
    "\n",
    "    ```\n",
    "    analogy('man','woman','king', vec)\n",
    "    'queen'\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Your code here ###\n",
    "\n",
    "# def analogy(a,b,c,v):\n",
    "   # Your code here\n",
    "    \n",
    "# analogy('man','woman','king', vec)\n",
    "# should return 'queen'\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
