{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22c05765-2e92-4539-8c41-592f03b44486",
   "metadata": {},
   "source": [
    "# Homework 4 - LING 6300"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07275f18-e23e-4ec3-9a92-f36eb452cd8a",
   "metadata": {},
   "source": [
    "## **Karan Praharaj**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17204be4-01e6-4ae2-88f1-6d681b9be911",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1cadc8a6-ac6c-487f-b879-15638bf4deba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.svm import LinearSVC\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import nltk\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f5148d-0f3f-417b-ae22-5ddc33bb57ed",
   "metadata": {},
   "source": [
    "For our training features, we will extract character n-grams (bigrams and trigrams) from the words in our datasets. This method of feature extraction is not linguistically rich, however, it can be argued that even something as simple as learning the patterns of character sequences can make for a decent naive model. This is just a hypothesis, and we will revisit this approach if the performance is less than satisfactory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfefd7e0-6d09-4dab-82ac-889253d1c18c",
   "metadata": {},
   "source": [
    "For example, the features generated from the word 'karan' would be:\n",
    "\n",
    "_'kar ara ran ka ar ra an'_\n",
    "\n",
    "We will string together these constituent patterns into a list, and treat it like a sentence of 'words'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e43a031a-fc1c-4634-a2a7-fd9d8df7399c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature extraction\n",
    "\n",
    "def constituents(word):\n",
    "    const_list = []\n",
    "    \n",
    "    ## adding trigram character combinations.\n",
    "    for i in range(0,len(word)-2):\n",
    "        const_list.append(word[i:i+3])\n",
    "    \n",
    "    ## adding all bigram character combinations.\n",
    "    for i in range(0,len(word)-1):\n",
    "        const_list.append(word[i:i+2])\n",
    "\n",
    "        \n",
    "    const_sent = ' '.join(const_list)\n",
    "    \n",
    "    return const_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd42f1fd-daf1-4811-8a7e-e0c40f55c374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function sets up a central pandas dataframe for all our data.\n",
    "\n",
    "data = {}\n",
    "\n",
    "def generate_table():\n",
    "    txtfiles = []\n",
    "    for file in glob.glob(\"hw4data/*.txt\"):\n",
    "        file_name = file.split('/')[-1].split('.txt')[0]\n",
    "        txtfiles.append(file_name)\n",
    "        data[file_name] = pd.read_csv(file, sep = '\\t', header = None, names = ['label', 'word'])\n",
    "        \n",
    "    # Lower-casing and word constituent generation.\n",
    "    for i in data:\n",
    "        data[i]['word'] = data[i]['word'].apply(lambda x: x.lower())\n",
    "        data[i]['word_constituents'] = data[i]['word'].apply(constituents)\n",
    "    \n",
    "    return data\n",
    "\n",
    "data = generate_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f053692-e198-41fc-b030-5c61533081f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = {}\n",
    "def generate_corpora():\n",
    "    for i in data:\n",
    "        corpus[i] = list(data[i]['word_constituents'])\n",
    "    \n",
    "    return corpus\n",
    "\n",
    "corpus = generate_corpora()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "395b5e2f-4fef-4844-b198-54ea1dabee65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing and pre-processing the data for training and evaluation. (90-10 split)\n",
    "\n",
    "X, y = {}, {}\n",
    "X_train, y_train = {}, {}\n",
    "X_train_cts = {}\n",
    "X_test, y_test = {}, {}\n",
    "X_test_cts = {}\n",
    "vectorizer = {}\n",
    "\n",
    "def data_splitter():\n",
    "    \n",
    "    for i in data:\n",
    "        X[i] = corpus[i]\n",
    "        y[i] = data[i]['label']\n",
    "        X_train[i], X_test[i], y_train[i], y_test[i] = train_test_split(X[i], y[i], train_size=0.90, test_size=0.10, random_state=101)\n",
    "        vectorizer[i] = CountVectorizer()\n",
    "        X_train_cts[i] = vectorizer[i].fit_transform(X_train[i])\n",
    "        X_test_cts[i] = vectorizer[i].transform(X_test[i])\n",
    "    \n",
    "    return X_train_cts, y_train, X_test_cts, y_test\n",
    "\n",
    "X_train, y_train, X_test, y_test = data_splitter()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1bc9bc4-f16b-40b9-b01a-401047f6c91d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET: de_v\n",
      "Training features shape:  (1643, 2545)\n",
      "Training labels shape:  (1643,)\n",
      "==========================================\n",
      "DATASET: es_v\n",
      "Training features shape:  (3469, 2595)\n",
      "Training labels shape:  (3469,)\n",
      "==========================================\n",
      "DATASET: fi_v\n",
      "Training features shape:  (6344, 3461)\n",
      "Training labels shape:  (6344,)\n",
      "==========================================\n",
      "DATASET: fi_na\n",
      "Training features shape:  (5399, 4402)\n",
      "Training labels shape:  (5399,)\n",
      "==========================================\n",
      "DATASET: de_n\n",
      "Training features shape:  (2307, 3780)\n",
      "Training labels shape:  (2307,)\n",
      "==========================================\n"
     ]
    }
   ],
   "source": [
    "## A sanity check to see that the number of feature sets are the same as the number of labels for each dataset.\n",
    "\n",
    "for i in X_train:\n",
    "    print(f\"DATASET: {i}\")\n",
    "    print(\"Training features shape: \", X_train[i].shape)\n",
    "    print(\"Training labels shape: \", y_train[i].shape)\n",
    "    print(\"==========================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99fb7aa-efc0-445c-bfb6-1468176daef9",
   "metadata": {},
   "source": [
    "**We will use LinearSVC as our first choice of model. If it ends up not delivering a good enough performance, we will switch to Multinomial Naive Bayes, or some type of Kernelized SVM.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a610975b-52fe-46ea-918c-871da8d31728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ TEST ACCURACY by dataset ================\n",
      "de_v : 0.7868852459016393\n",
      "es_v : 0.9326424870466321\n",
      "fi_v : 0.9404255319148936\n",
      "fi_na : 0.7216666666666667\n",
      "de_n : 0.7042801556420234\n"
     ]
    }
   ],
   "source": [
    "# Training and then generating test accuracy scores for each dataset.\n",
    "\n",
    "clf = {}\n",
    "print(\"================ TEST ACCURACY by dataset ================\")\n",
    "for i in data:\n",
    "    clf[i] = LinearSVC(random_state=42, tol=1e-5, max_iter=1000, C=0.1)\n",
    "    clf[i].fit(X_train[i], y_train[i])\n",
    "    y_pred = clf[i].predict(X_test[i])\n",
    "    print(i, \":\", accuracy_score(y_pred, y_test[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1744ab7-866f-4360-ac85-8cd5c7b37d5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a08a95c-2bb4-4b00-a76c-e9ea119205ee",
   "metadata": {},
   "source": [
    "**Even with a relatively naive approach, our accuracies are pretty respectable. Especially when compared to the state-of-the-art scores of 80% for _de_n_ and 99% for _es_v_.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d864df02-d177-4a21-81a8-f7273691e88b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
